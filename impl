bits 64
default rel


%include "cabna/conv"


global _start
global alloc_task
global sched_task
global supply_retval
global exec_avail
global free_pet__exec_avail

%ifdef statistics_collection
global print_stats
extern printf
%endif

extern main
extern bug


; It is assumed that all Cabna code and data is located in 32-bit address space,
; which allows assuming Cabna addresses fit in 32 bits.  This is taken advantage
; of to use 32-bit operations instead of 64-bit, as the processor manufacturers
; recommend, when possible, for optimization, and also to fit two pointers in
; r15.  Cabna is still a 64-bit system intended for 64-bit use, and 64-bit
; addresses and operations are used for users' stuff and strucs that might be
; allocated in 64-bit space.


; Users' responsibilities:
;  1) Task strucs must not be in more than one stack at a time.
;  2) Task strucs must not be in the same stack more than once.
;  3) The memory allocated by the OS for task strucs must never be returned to
;     the OS.
; It is assumed that users obey these responsibilities.




section .text


proc alloc_task:
  ; Pop a free task struc, from the thread's allocatable-tasks stack, or from
  ; the shared stack, or allocate more memory from the OS, and return to ret_rsi
  ; with task pointer in arg1_rdi.  Each thread has its own private
  ; allocatable-tasks stack, so if that's used, concurrency-safety is not
  ; needed.  If the shared stack is used, concurrencry-safety is needed.
  ; Used registers: rax, rcx, rdx, rsi, rdi, r8, r9, r10, r11.

  stat add qword [t_stats + stats.allocated], 1

  mov arg1_rdi, [alloc_s_r15d + stack.head]
  test arg1_rdi, arg1_rdi
  unlikely jz .from_shared

  sub dword [alloc_s_r15d + stack.size], 1  ; 32-bit alright.
.from_private:
  ; The next, which might be null, becomes the head.
  mov rax, [arg1_rdi + task.next]
  mov [alloc_s_r15d + stack.head], rax
  mov qword [arg1_rdi + task.next], 0
  jmp_ind ret_rsi

.from_shared_spin:
  pause
  stat add qword [t_stats + stats.exclusion], 1
.from_shared:
  bt dword [s_alloc_s + stack.next], 0
  jc .from_shared_spin  ; Another thread already has it locked.
  ; Try to lock the stack for our use.
  lock bts dword [s_alloc_s + stack.next], 0
  jc .from_shared_spin  ; Another thread just locked it.

  ; Other threads cannot concurrently execute the rest of this procedure.  This
  ; is what enables the concurrency-safety.
  mov rax, [s_alloc_s + stack.head]
  test rax, rax
  unlikely jz .get_new_space

.have_space:
  mov arg1_rdi, rax  ; Head is return value.
  mov ecx, private_free_tasks_max  ; 32-bit alright.
.take_shared:
  mov rdx, rax
  mov rax, [rax + task.next]
  sub ecx, 1
  jz .shared_done
  test rax, rax
  jnz .take_shared
.shared_done:
  ; The untaken, which might be null, becomes the head.
  mov [s_alloc_s + stack.head], rax
  ; Unlock the shared stack.
  and dword [s_alloc_s + stack.next], -2  ; Clear lock bit.
  ; Push-at-once in private stack.
  ;;;mov [alloc_s_r15d + stack.head], arg1_rdi  ; Not necessary.
  mov qword [rdx + task.next], 0
  mov eax, private_free_tasks_max - 1  ; 32-bit alright.
  sub eax, ecx
  mov [alloc_s_r15d + stack.size], eax  ; 1 less because head is returned.
  jmp .from_private

.get_new_space:
  ; Get some more space from the OS, divide it into blocks of task strucs, link
  ; them together, and jump to .have_space.  The fields of the new strucs are
  ; null because mmap zeros, as needed for future operations.

  stat add qword [t_stats + stats.mmap], 1

  ; Save registers.
  ;mov ?, r9
  ;mov ?, r8
  ;mov ?, r10
  ;mov ?, rdx
  mov [s_alloc_s + stack.head], ret_rsi  ; Just a temporary location.
  ;mov ?, rdi
  ;mov ?, rax  ; syscall destroys
  ;mov ?, rcx  ; syscall destroys
  ;mov ?, r11  ; syscall destroys

  ; mmap a space.
  xor r9d, r9d                     ; pgoffset
  mov r8, dword -1                 ; fd
  mov r10d, 0x22                   ; flags = MAP_PRIVATE | MAP_ANONYMOUS
  mov edx, 3                       ; prot = PROT_READ | PROT_WRITE
  mov esi, mmap_tasks * task_size  ; length
  xor edi, edi                     ; addr = NULL
  mov eax, 9                       ; mmap syscall number
  syscall
  cmp rax, -4096
  ja .mmap_failed

  ; Divide remaining space into task strucs, and link together.
  add rsi, rax  ; End of the space.
  sub rsi, task_size  ; Tail struc is the last block.
  xor edx, edx
.divide_and_link:
  mov [rsi + task.next], rdx
  mov rdx, rsi
  sub rsi, task_size
  cmp rsi, rax
  jae .divide_and_link
  mov ret_rsi, [s_alloc_s + stack.head]  ; Restore saved.
  ;;;mov [s_alloc_s + stack.head], rax  ; Not necessary.
  jmp .have_space

.mmap_failed:
  ; TODO: Negated error code is in rax, print to stderr, and do something...
  jmp bug




proc supply_retval:
  ; Return a value to the currently executing task's waiting receiver task,
  ; argument in arg1_rdi.  If the receiver task gets its final value, push the
  ; task for execution.  Return to ret_rsi.
  ; Used registers: rax, rdx, rsi, rdi.

  ; No other threads will access our currently executing task struc, so we can
  ; access it without concurrency concern.
  mov rax, [cet_r14 + task.rcvr]
  mov edx, [cet_r14 + task.ridx]  ; 32-bit alright.
  ; No other threads will access this field in rcvr, so we can access it without
  ; concurrency concern.
  mov [rax + task.args + 8 * rdx], arg1_rdi
  ; Other threads will access the .need field, so we must atomically decrement
  ; it.
  lock sub dword [rax + task.need], 1  ; 32-bit alright.
  cmovz arg1_rdi, rax  ; For sched_task.
  ; Supplied the final needed argument to rcvr, so push rcvr for execution.
  jz sched_task  ; ret_rsi already set.
  ; Receiver needs more.
  jmp_ind ret_rsi




proc sched_task:
  ; Push a task in the thread's executable-tasks stack, or if the stack already
  ; has at least one task and another thread needs a task, give the task to the
  ; needy thread.  Argument in arg1_rdi, and return to ret_rsi.
  ; Used registers: rax, rdx, rsi, rdi.

  stat add qword [t_stats + stats.sched_calls], 1

  mov rdx, exec_s_r15
  shr rdx, 32

  mov rax, [edx + stack.head]
  test rax, rax
  jz .in_mine_empty

  ; Check if another thread needs a task.
  mov eax, edx
.another:
  mov eax, [eax + stack.next]  ; 32-bit alright.
  cmp eax, edx
  je .in_mine  ; Didn't find a needy thread.
  bt dword [eax + stack.need], 0
  jnc .another
  lock btr dword [eax + stack.need], 0
  jnc .another
  mov [eax + stack.need], arg1_rdi
  jmp_ind ret_rsi

.in_mine:
  mov rax, [edx + stack.head]
  mov [arg1_rdi + task.next], rax
.in_mine_empty:
  mov [edx + stack.head], arg1_rdi

%ifdef statistics_collection
  add qword [ts_stats + edx - ts_exec_ss + stats.sched_mine], 1
  mov rax, [edx + stack.size]
  add rax, 1
  mov [edx + stack.size], rax
  cmp rax, [ts_stats + edx - ts_exec_ss + stats.exec_s_max]
  jbe .not_max
  mov [ts_stats + edx - ts_exec_ss + stats.exec_s_max], rax
.not_max:
%endif

  jmp_ind ret_rsi




proc free_pet__exec_avail:
  ; Free the previously executing task by pushing it in the thread's
  ; allocatable-tasks stack, or in the shared stack if the thread's stack
  ; reached the max, and continue into exec_avail.  Each thread has its own
  ; private allocatable-tasks stack, so if that's used, concurrency-safety is
  ; not needed.  If the shared stack is used, concurrency-safety is needed.
  ; Used registers: rax, and those of exec_avail.

  stat add qword [t_stats + stats.freed], 1

  mov eax, [alloc_s_r15d + stack.size]  ; 32-bit alright.
  cmp eax, private_free_tasks_max
  jae .in_shared

  add eax, 1
  mov [alloc_s_r15d + stack.size], eax  ; 32-bit alright.
  mov rax, [alloc_s_r15d + stack.head]
  mov [cet_r14 + task.next], rax
  mov [alloc_s_r15d + stack.head], cet_r14
  jmp exec_avail

.in_shared_spin:
  pause
  stat add qword [t_stats + stats.exclusion], 1
.in_shared:
  bt dword [s_alloc_s + stack.next], 0
  jc .in_shared_spin  ; Another thread already has it locked.
  ; Try to lock the stack for our use.
  lock bts dword [s_alloc_s + stack.next], 0
  jc .in_shared_spin  ; Another thread just locked it.
  ; Other threads cannot concurrently execute the rest of this procedure.  This
  ; is what enables the concurrency-safety.
  mov rax, [s_alloc_s + stack.head]
  mov [cet_r14 + task.next], rax
  mov [s_alloc_s + stack.head], cet_r14
  ; Unlock the shared stack.
  and dword [s_alloc_s + stack.next], -2  ; Clear lock bit.
  jmp exec_avail




proc exec_avail:
  ; Pop a task from the thread's executable-tasks stack, or indicate that the
  ; thread needs a task and wait for one, then execute the task with cet_r14 set
  ; to the task.
  ; Used registers: rax, rdx, r14

  stat add qword [t_stats + stats.execute_calls], 1

  mov rdx, exec_s_r15
  shr rdx, 32

  mov cet_r14, [edx + stack.head]
  test cet_r14, cet_r14
  jz .empty

  stat sub qword [edx + stack.size], 1
  stat add qword [t_stats + stats.executed_mine], 1

  ; The next, which might be null, becomes the head.
  mov rax, [cet_r14 + task.next]
  mov [edx + stack.head], rax
  mov qword [cet_r14 + task.next], 0
  ; Execute the task's instructions.  Tasks are responsible for giving control
  ; back to exec_evail.
  jmp_ind [cet_r14 + task.exec]

.empty:
  mov qword [edx + stack.need], 1
.wait:
  pause  ; Doesn't seem to affect speed. Hopefully reduces power consumption.
  stat add qword [t_stats + stats.wait], 1
  mov cet_r14, [edx + stack.need]
  test cet_r14, -2
  jz .wait
  ; Execute the task's instructions.  Tasks are responsible for giving control
  ; back to exec_evail.
  jmp_ind [cet_r14 + task.exec]




proc _start:
  ; Entry point.

  xor ebp, ebp  ; Unix ABI says to do this.
  ; TODO?: Unix ABI says rdx is a function pointer to register with atexit.

  mov ebx, amount_threads

.loop:
  sub ebx, 1
  jz .done

  ; Get space for the threads' stacks, via mmap.
  xor r9d, r9d              ; pgoffset
  mov r8, dword -1          ; fd
  mov r10d, 0x20022         ; flags = MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK
  mov edx, 3                ; prot = PROT_READ | PROT_WRITE
  mov esi, call_stack_size  ; length
  xor edi, edi              ; addr = NULL
  mov eax, 9                ; mmap syscall number
  syscall
  cmp rax, -4096
  ja .mmap_failed

  ; Create the threads using clone.
  xor r8d, r8d         ; tls = NULL
  xor r10d, r10d       ; ctid = NULL
  xor edx, edx         ; ptid = NULL
  add rax, call_stack_size
  mov rsi, rax         ; child_stack = end of mmap'ed memory
  mov edi, 0x80012F00  ; flags = CLONE_FILES | CLONE_FS | CLONE_IO | CLONE_PTRACE
                       ;         | CLONE_SIGHAND | CLONE_THREAD | CLONE_VM
  mov eax, 56          ; clone syscall number
  syscall
  cmp rax, -4096
  ja .clone_failed

  test rax, rax
  jnz .loop  ; Caller of clone does this.

  ; New threads do this.

.done:
  ; Set the threads' executable-tasks and allocatable-tasks stacks register.
  ; Multiply thread ID by 128: the offset of a thread's stacks' addresses.
  shl ebx, 7
  ; The executable-tasks stack.
  lea r15d, [ts_exec_ss + ebx]  ; Assumes it's in 32-bit space.
  shl r15, 32  ; It lives in the high 32 bits.
  ; The allocatable-tasks stack.
  lea eax, [ts_alloc_ss + ebx]  ; Assumes it's in 32-bit space.
  or r15, rax  ; It lives in the low 32 bits.
  ; Begin executing the user's program.
  test ebx, ebx
  jz main  ; Initial thread, thread 0, does this.
  jmp exec_avail  ; Other threads do this.  TODO?: Delay a bit first?

.mmap_failed:
  ; TODO: Negated error code is in rax, print to stderr, and do something...
  jmp bug

.clone_failed:
  ; TODO: Negated error code is in rax, print to stderr, and do something...
  jmp bug




%ifdef statistics_collection

proc print_stats:
  ; Print collected statistics for each thread, and print the totals of all
  ; threads.  Called directly via call not via exec_avail, so the things having
  ; statistics collected are not used to execute it.

  ; Temporary struc for the totals of all threads.
  sub rsp, stats_size
  ;;; [rsp + stats.exec_s_max] not totaled.
  mov qword [rsp + stats.execute_calls], 0
  mov qword [rsp + stats.executed_mine], 0
  mov qword [rsp + stats.wait], 0
  mov qword [rsp + stats.sched_calls], 0
  mov qword [rsp + stats.sched_mine], 0
  mov qword [rsp + stats.allocated], 0
  mov qword [rsp + stats.freed], 0
  mov qword [rsp + stats.exclusion], 0
  mov qword [rsp + stats.mmap], 0

  mov ebx, amount_threads - 1
.loop:
  mov esi, ebx
  mov edi, stats_tid_fmtstr
  mov eax, 0
  call printf
  ; Multiply thread ID by 128: the offset of the thread's stats address.
  shl ebx, 7

  mov rsi, [ts_stats + ebx + stats.exec_s_max]
  mov edi, stats_exec_s_max_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.execute_calls]
  add [rsp + stats.execute_calls], rsi
  mov edi, stats_execute_calls_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.executed_mine]
  add [rsp + stats.executed_mine], rsi
  mov edi, stats_executed_mine_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.wait]
  add [rsp + stats.wait], rsi
  mov edi, stats_wait_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.sched_calls]
  add [rsp + stats.sched_calls], rsi
  mov edi, stats_sched_calls_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.sched_mine]
  add [rsp + stats.sched_mine], rsi
  mov edi, stats_sched_mine_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.allocated]
  add [rsp + stats.allocated], rsi
  mov edi, stats_allocated_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.freed]
  add [rsp + stats.freed], rsi
  mov edi, stats_freed_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.exclusion]
  add [rsp + stats.exclusion], rsi
  mov edi, stats_exclusion_fmtstr
  mov eax, 0
  call printf

  mov rsi, [ts_stats + ebx + stats.mmap]
  add [rsp + stats.mmap], rsi
  mov edi, stats_mmap_fmtstr
  mov eax, 0
  call printf

  mov eax, mmap_tasks * task_size / 1024
  mov rsi, [ts_stats + ebx + stats.mmap]
  mul rsi
  mov rsi, rdx
  mov rdx, rax
  mov edi, stats_mmap_kb_fmtstr
  mov eax, 0
  call printf

  shr ebx, 7
  sub ebx, 1
  jnc .loop

  mov esi, amount_threads
  mov edi, stats_total_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.execute_calls]
  mov edi, stats_execute_calls_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.executed_mine]
  mov edi, stats_executed_mine_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.wait]
  mov edi, stats_wait_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.sched_calls]
  mov edi, stats_sched_calls_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.sched_mine]
  mov edi, stats_sched_mine_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.allocated]
  mov edi, stats_allocated_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.freed]
  mov edi, stats_freed_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.exclusion]
  mov edi, stats_exclusion_fmtstr
  mov eax, 0
  call printf

  mov rsi, [rsp + stats.mmap]
  mov edi, stats_mmap_fmtstr
  mov eax, 0
  call printf

  mov eax, mmap_tasks * task_size / 1024
  mov rsi, [rsp + stats.mmap]
  mul rsi
  mov rsi, rdx
  mov rdx, rax
  mov edi, stats_mmap_kb_fmtstr
  mov eax, 0
  call printf

  add rsp, stats_size
  ret

%endif




section .data  align=128

; Strucs are aligned at 128-byte boundary because this makes the processor bus
; locking used for atomic operations more efficient.  It also makes the needed
; 8-byte alignment for atomic field access.  _start and statistics collection
; also take advantage of it.


; Threads' executable-tasks stacks.

ts_exec_ss:

%assign n 0
%rep amount_threads

t%[n]_exec_s:

%assign n  (n + 1) % amount_threads  ; Last's .next = first.

  istruc stack
    at stack.head, dq 0
    at stack.next, dq t%[n]_exec_s
    at stack.size, dq 0  ; Only used when statistics collection is enabled.
    at stack.need, dq 0
  iend

align 128, db 0

%endrep




; Shared allocatable-tasks stack.

s_alloc_s:

  istruc stack
    at stack.head, dq 0
    at stack.next, dq 0  ; Only used for lock bit.
    at stack.size, dq 0  ; Not used.
    at stack.need, dq 0  ; Not used.
  iend

align 128, db 0




; Threads' private allocatable-tasks stacks.

ts_alloc_ss:

%assign n 0
%rep amount_threads

t%[n]_alloc_s:

  istruc stack
    at stack.head, dq t%[n]_alloc_s_head
    at stack.next, dq 0  ; Not used.
    at stack.size, dq statically_allocated_free_tasks
    at stack.need, dq 0  ; Not used.
  iend

align 128, db 0

%assign n  n + 1
%endrep




; Statically-allocated and initialized task strucs for the allocatable-tasks
; stacks.  Task strucs are 128-bytes long, which preserves their 128-byte
; alignment when they are contiguously located like below.

%assign n 0
%rep amount_threads

t%[n]_alloc_s_head:

  istruc task
    at task.next,    dq t%[n]_alloc_s_1
    at task.rcvr,    dq 0
    at task.ridx,    dq 0
    at task.need,    dq 0
    at task.exec,    dq 0
    at task.arg1,    dq 0
    at task.arg2,    dq 0
    at task.arg3,    dq 0
    at task.arg4,    dq 0
    at task.arg5,    dq 0
    at task.arg6,    dq 0
    at task.arg7,    dq 0
    at task.arg8,    dq 0
    at task.arg9,    dq 0
    at task.arg10,   dq 0
    at task.arg11,   dq 0
  iend


t%[n]_alloc_s_1:

%assign a 2
%rep statically_allocated_free_tasks - 2

  istruc task
    at task.next,    dq t%[n]_alloc_s_%[a]
    at task.rcvr,    dq 0
    at task.ridx,    dq 0
    at task.need,    dq 0
    at task.exec,    dq 0
    at task.arg1,    dq 0
    at task.arg2,    dq 0
    at task.arg3,    dq 0
    at task.arg4,    dq 0
    at task.arg5,    dq 0
    at task.arg6,    dq 0
    at task.arg7,    dq 0
    at task.arg8,    dq 0
    at task.arg9,    dq 0
    at task.arg10,   dq 0
    at task.arg11,   dq 0
  iend

t%[n]_alloc_s_%[a]:

%assign a  a + 1
%endrep


  istruc task
    at task.next,    dq 0
    at task.rcvr,    dq 0
    at task.ridx,    dq 0
    at task.need,    dq 0
    at task.exec,    dq 0
    at task.arg1,    dq 0
    at task.arg2,    dq 0
    at task.arg3,    dq 0
    at task.arg4,    dq 0
    at task.arg5,    dq 0
    at task.arg6,    dq 0
    at task.arg7,    dq 0
    at task.arg8,    dq 0
    at task.arg9,    dq 0
    at task.arg10,   dq 0
    at task.arg11,   dq 0
  iend

%assign n  n + 1
%endrep




%ifdef statistics_collection

ts_stats:

%rep amount_threads

  istruc stats
    at stats.exec_s_max,     dq 0
    at stats.execute_calls,  dq 0
    at stats.executed_mine,  dq 0
    at stats.wait,           dq 0
    at stats.sched_calls,    dq 0
    at stats.sched_mine,     dq 0
    at stats.allocated,      dq 0
    at stats.freed,          dq 0
    at stats.exclusion,      dq 0
    at stats.mmap,           dq 0
  iend

align 128, db 0  ; Necessary because the exec and alloc stacks are used to find
                 ; each thread's stats struc.
%endrep

stats_tid_fmtstr:            db `Thread %lu:\n`,0
stats_exec_s_max_fmtstr:     db `  exec_s max:    %llu\n`,0
stats_execute_calls_fmtstr:  db `  execute calls: %llu\n`,0
stats_executed_mine_fmtstr:  db `  executed mine: %llu\n`,0
stats_wait_fmtstr:           db `  waits:         %llu\n`,0
stats_sched_calls_fmtstr:    db `  push calls:    %llu\n`,0
stats_sched_mine_fmtstr:     db `  pushed mine:   %llu\n`,0
stats_allocated_fmtstr:      db `  allocated:     %llu\n`,0
stats_freed_fmtstr:          db `  freed:         %llu\n`,0
stats_exclusion_fmtstr:      db `  exclusions:    %llu\n`,0
stats_mmap_fmtstr:           db `  mmaps:         %llu\n`,0
stats_mmap_kb_fmtstr:        db `  KB mmaped:     %llu,%llu\n`,0
stats_total_fmtstr:          db `Totals of %lu threads:\n`,0

%endif
